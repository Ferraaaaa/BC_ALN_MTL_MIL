{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from typing import Sequence\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from mmcv.cnn import build_norm_layer\n",
    "from mmcv.cnn.bricks.transformer import FFN\n",
    "from mmcv.runner.base_module import BaseModule, ModuleList\n",
    "from mmcls.utils import get_root_logger\n",
    "\n",
    "from mmcls.models.builder import BACKBONES\n",
    "from mmcls.models.utils import MultiheadAttention, PatchEmbed, to_2tuple\n",
    "from mmcls.models.backbones.base_backbone import BaseBackbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(BaseModule):\n",
    "    \"\"\"Implements one encoder layer in Vision Transformer.\n",
    "\n",
    "    Args:\n",
    "        embed_dims (int): The feature dimension\n",
    "        num_heads (int): Parallel attention heads\n",
    "        feedforward_channels (int): The hidden dimension for FFNs\n",
    "        drop_rate (float): Probability of an element to be zeroed\n",
    "            after the feed forward layer. Defaults to 0.\n",
    "        attn_drop_rate (float): The drop out rate for attention output weights.\n",
    "            Defaults to 0.\n",
    "        drop_path_rate (float): Stochastic depth rate. Defaults to 0.\n",
    "        num_fcs (int): The number of fully-connected layers for FFNs.\n",
    "            Defaults to 2.\n",
    "        qkv_bias (bool): enable bias for qkv if True. Defaults to True.\n",
    "        act_cfg (dict): The activation config for FFNs.\n",
    "            Defaluts to ``dict(type='GELU')``.\n",
    "        norm_cfg (dict): Config dict for normalization layer.\n",
    "            Defaults to ``dict(type='LN')``.\n",
    "        init_cfg (dict, optional): Initialization config dict.\n",
    "            Defaults to None.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 embed_dims,\n",
    "                 num_heads,\n",
    "                 feedforward_channels,\n",
    "                 drop_rate=0.,\n",
    "                 attn_drop_rate=0.,\n",
    "                 drop_path_rate=0.,\n",
    "                 num_fcs=2,\n",
    "                 qkv_bias=True,\n",
    "                 act_cfg=dict(type='GELU'),\n",
    "                 norm_cfg=dict(type='LN'),\n",
    "                 init_cfg=None):\n",
    "        super(TransformerEncoderLayer, self).__init__(init_cfg=init_cfg)\n",
    "\n",
    "        self.embed_dims = embed_dims\n",
    "\n",
    "        self.norm1_name, norm1 = build_norm_layer(\n",
    "            norm_cfg, self.embed_dims, postfix=1)\n",
    "        self.add_module(self.norm1_name, norm1)\n",
    "\n",
    "        self.attn = MultiheadAttention(\n",
    "            embed_dims=embed_dims,\n",
    "            num_heads=num_heads,\n",
    "            attn_drop=attn_drop_rate,\n",
    "            proj_drop=drop_rate,\n",
    "            dropout_layer=dict(type='DropPath', drop_prob=drop_path_rate),\n",
    "            qkv_bias=qkv_bias)\n",
    "\n",
    "        self.norm2_name, norm2 = build_norm_layer(\n",
    "            norm_cfg, self.embed_dims, postfix=2)\n",
    "        self.add_module(self.norm2_name, norm2)\n",
    "\n",
    "        self.ffn = FFN(\n",
    "            embed_dims=embed_dims,\n",
    "            feedforward_channels=feedforward_channels,\n",
    "            num_fcs=num_fcs,\n",
    "            ffn_drop=drop_rate,\n",
    "            dropout_layer=dict(type='DropPath', drop_prob=drop_path_rate),\n",
    "            act_cfg=act_cfg)\n",
    "\n",
    "    @property\n",
    "    def norm1(self):\n",
    "        return getattr(self, self.norm1_name)\n",
    "\n",
    "    @property\n",
    "    def norm2(self):\n",
    "        return getattr(self, self.norm2_name)\n",
    "\n",
    "    def init_weights(self):\n",
    "        super(TransformerEncoderLayer, self).init_weights()\n",
    "        for m in self.ffn.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                nn.init.normal_(m.bias, std=1e-6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = self.ffn(self.norm2(x), identity=x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(BaseBackbone):\n",
    "    \"\"\"Vision Transformer.\n",
    "\n",
    "    A PyTorch implement of : `An Image is Worth 16x16 Words:\n",
    "    Transformers for Image Recognition at\n",
    "    Scale<https://arxiv.org/abs/2010.11929>`_\n",
    "\n",
    "    Args:\n",
    "        arch (str | dict): Vision Transformer architecture\n",
    "            Default: 'b'\n",
    "        img_size (int | tuple): Input image size\n",
    "        patch_size (int | tuple): The patch size\n",
    "        out_indices (Sequence | int): Output from which stages.\n",
    "            Defaults to -1, means the last stage.\n",
    "        drop_rate (float): Probability of an element to be zeroed.\n",
    "            Defaults to 0.\n",
    "        drop_path_rate (float): stochastic depth rate. Defaults to 0.\n",
    "        norm_cfg (dict): Config dict for normalization layer.\n",
    "            Defaults to ``dict(type='LN')``.\n",
    "        final_norm (bool): Whether to add a additional layer to normalize\n",
    "            final feature map. Defaults to True.\n",
    "        output_cls_token (bool): Whether output the cls_token. If set True,\n",
    "            `with_cls_token` must be True. Defaults to True.\n",
    "        interpolate_mode (str): Select the interpolate mode for position\n",
    "            embeding vector resize. Defaults to \"bicubic\".\n",
    "        patch_cfg (dict): Configs of patch embeding. Defaults to an empty dict.\n",
    "        layer_cfgs (Sequence | dict): Configs of each transformer layer in\n",
    "            encoder. Defaults to an empty dict.\n",
    "        init_cfg (dict, optional): Initialization config dict.\n",
    "            Defaults to None.\n",
    "    \"\"\"\n",
    "    arch_zoo = {\n",
    "        **dict.fromkeys(\n",
    "            ['s', 'small'], {\n",
    "                'embed_dims': 768,\n",
    "                'num_layers': 8,\n",
    "                'num_heads': 8,\n",
    "                'feedforward_channels': 768 * 3,\n",
    "                'qkv_bias': False\n",
    "            }),\n",
    "        **dict.fromkeys(\n",
    "            ['b', 'base'], {\n",
    "                'embed_dims': 256,\n",
    "                'num_layers': 12,\n",
    "                'num_heads': 4,\n",
    "                'feedforward_channels': 256*4\n",
    "            }),\n",
    "        **dict.fromkeys(\n",
    "            ['l', 'large'], {\n",
    "                'embed_dims': 1024,\n",
    "                'num_layers': 24,\n",
    "                'num_heads': 16,\n",
    "                'feedforward_channels': 4096\n",
    "            }),\n",
    "    }\n",
    "\n",
    "    def __init__(self,\n",
    "                 arch='b',\n",
    "                 patch_num=200,\n",
    "                 out_indices=-1,\n",
    "                 drop_rate=0.,\n",
    "                 drop_path_rate=0.,\n",
    "                 norm_cfg=dict(type='LN', eps=1e-6),\n",
    "                 final_norm=True,\n",
    "                 output_cls_token=True,\n",
    "                 interpolate_mode='bicubic',\n",
    "                 patch_cfg=dict(),\n",
    "                 layer_cfgs=dict(),\n",
    "                 init_cfg=None):\n",
    "        super(VisionTransformer, self).__init__(init_cfg)\n",
    "\n",
    "        if isinstance(arch, str):\n",
    "            arch = arch.lower()\n",
    "            assert arch in set(self.arch_zoo), \\\n",
    "                f'Arch {arch} is not in default archs {set(self.arch_zoo)}'\n",
    "            self.arch_settings = self.arch_zoo[arch]\n",
    "        else:\n",
    "            essential_keys = {\n",
    "                'embed_dims', 'num_layers', 'num_heads', 'feedforward_channels'\n",
    "            }\n",
    "            assert isinstance(arch, dict) and set(arch) == essential_keys, \\\n",
    "                f'Custom arch needs a dict with keys {essential_keys}'\n",
    "            self.arch_settings = arch\n",
    "\n",
    "        self.embed_dims = self.arch_settings['embed_dims']\n",
    "        self.num_layers = self.arch_settings['num_layers']\n",
    "        \n",
    "\n",
    "        # Set patch embedding\n",
    "        self.patch_num = patch_num\n",
    "\n",
    "        # Set cls token\n",
    "        self.output_cls_token = output_cls_token\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, self.embed_dims))\n",
    "\n",
    "        # Set position embedding\n",
    "        self.interpolate_mode = interpolate_mode\n",
    "        self.pos_embed = nn.Parameter(\n",
    "            torch.zeros(1, patch_num + 1, self.embed_dims))\n",
    "        self.drop_after_pos = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        if isinstance(out_indices, int):\n",
    "            out_indices = [out_indices]\n",
    "        assert isinstance(out_indices, Sequence), \\\n",
    "            f'\"out_indices\" must by a sequence or int, ' \\\n",
    "            f'get {type(out_indices)} instead.'\n",
    "        for i, index in enumerate(out_indices):\n",
    "            if index < 0:\n",
    "                out_indices[i] = self.num_layers + index\n",
    "            assert 0 <= out_indices[i] <= self.num_layers, \\\n",
    "                f'Invalid out_indices {index}'\n",
    "        self.out_indices = out_indices\n",
    "\n",
    "        # stochastic depth decay rule\n",
    "        dpr = np.linspace(0, drop_path_rate, self.arch_settings['num_layers'])\n",
    "\n",
    "        self.layers = ModuleList()\n",
    "        if isinstance(layer_cfgs, dict):\n",
    "            layer_cfgs = [layer_cfgs] * self.num_layers\n",
    "        for i in range(self.num_layers):\n",
    "            _layer_cfg = dict(\n",
    "                embed_dims=self.embed_dims,\n",
    "                num_heads=self.arch_settings['num_heads'],\n",
    "                feedforward_channels=self.\n",
    "                arch_settings['feedforward_channels'],\n",
    "                drop_rate=drop_rate,\n",
    "                drop_path_rate=dpr[i],\n",
    "                qkv_bias=self.arch_settings.get('qkv_bias', True),\n",
    "                norm_cfg=norm_cfg)\n",
    "            _layer_cfg.update(layer_cfgs[i])\n",
    "            self.layers.append(TransformerEncoderLayer(**_layer_cfg))\n",
    "\n",
    "        self.final_norm = final_norm\n",
    "        if final_norm:\n",
    "            self.norm1_name, norm1 = build_norm_layer(\n",
    "                norm_cfg, self.embed_dims, postfix=1)\n",
    "            self.add_module(self.norm1_name, norm1)\n",
    "\n",
    "    @property\n",
    "    def norm1(self):\n",
    "        return getattr(self, self.norm1_name)\n",
    "\n",
    "    def init_weights(self):\n",
    "        # Suppress default init if use pretrained model.\n",
    "        # And use custom load_checkpoint function to load checkpoint.\n",
    "        if (isinstance(self.init_cfg, dict)\n",
    "                and self.init_cfg['type'] == 'Pretrained'):\n",
    "            init_cfg = deepcopy(self.init_cfg)\n",
    "            init_cfg.pop('type')\n",
    "            self._load_checkpoint(**init_cfg)\n",
    "        else:\n",
    "            super(VisionTransformer, self).init_weights()\n",
    "            # Modified from ClassyVision\n",
    "            nn.init.normal_(self.pos_embed, std=0.02)\n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "       \n",
    "        #patch_resolution = self.patch_embed.patches_resolution\n",
    "\n",
    "        # stole cls_tokens impl from Phil Wang, thanks\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x = x + self.pos_embed\n",
    "        x = self.drop_after_pos(x)\n",
    "\n",
    "        print(x.shape)\n",
    "        \n",
    "        outs = []\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "\n",
    "            if i == len(self.layers) - 1 and self.final_norm:\n",
    "                x = self.norm1(x)\n",
    "\n",
    "            #print(x.shape)\n",
    "\n",
    "            if i in self.out_indices:\n",
    "               \n",
    "                \n",
    "                #patch_token = x[:, 1:].reshape(B, *patch_resolution, C)\n",
    "                patch_token = x[:, 1:]\n",
    "                print(patch_token.shape)\n",
    "\n",
    "                patch_token = patch_token.permute(0, 2, 1)\n",
    "\n",
    "                print(patch_token.shape)\n",
    "\n",
    "                cls_token = x[:, 0]\n",
    "\n",
    "                print(cls_token.shape)\n",
    "                \n",
    "                if self.output_cls_token:\n",
    "                    out = [patch_token, cls_token]\n",
    "                else:\n",
    "                    out = patch_token\n",
    "                outs.append(out)\n",
    "\n",
    "        return tuple(outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[50., 50., 50.,  ..., 50., 50., 50.],\n",
      "          [50., 50., 50.,  ..., 50., 50., 50.],\n",
      "          [50., 50., 50.,  ..., 50., 50., 50.],\n",
      "          ...,\n",
      "          [50., 50., 50.,  ..., 50., 50., 50.],\n",
      "          [50., 50., 50.,  ..., 50., 50., 50.],\n",
      "          [50., 50., 50.,  ..., 50., 50., 50.]],\n",
      "\n",
      "         [[50., 50., 50.,  ..., 50., 50., 50.],\n",
      "          [50., 50., 50.,  ..., 50., 50., 50.],\n",
      "          [50., 50., 50.,  ..., 50., 50., 50.],\n",
      "          ...,\n",
      "          [50., 50., 50.,  ..., 50., 50., 50.],\n",
      "          [50., 50., 50.,  ..., 50., 50., 50.],\n",
      "          [50., 50., 50.,  ..., 50., 50., 50.]],\n",
      "\n",
      "         [[50., 50., 50.,  ..., 50., 50., 50.],\n",
      "          [50., 50., 50.,  ..., 50., 50., 50.],\n",
      "          [50., 50., 50.,  ..., 50., 50., 50.],\n",
      "          ...,\n",
      "          [50., 50., 50.,  ..., 50., 50., 50.],\n",
      "          [50., 50., 50.,  ..., 50., 50., 50.],\n",
      "          [50., 50., 50.,  ..., 50., 50., 50.]]]])\n",
      "tensor([[[50., 50., 50.,  ..., 50., 50., 50.],\n",
      "         [50., 50., 50.,  ..., 50., 50., 50.],\n",
      "         [50., 50., 50.,  ..., 50., 50., 50.],\n",
      "         ...,\n",
      "         [50., 50., 50.,  ..., 50., 50., 50.],\n",
      "         [50., 50., 50.,  ..., 50., 50., 50.],\n",
      "         [50., 50., 50.,  ..., 50., 50., 50.]]])\n"
     ]
    }
   ],
   "source": [
    "img_a = torch.full([1,3,224,224],50.0)\n",
    "print(img_a)\n",
    "img_b = torch.full([1,200,256],50.0)\n",
    "print(img_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 201, 256])\n",
      "torch.Size([1, 200, 256])\n",
      "torch.Size([1, 256, 200])\n",
      "torch.Size([1, 256])\n"
     ]
    }
   ],
   "source": [
    "v = VisionTransformer()\n",
    "\n",
    "result = v.forward(img_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "[11]\n"
     ]
    }
   ],
   "source": [
    "print(v.num_layers)\n",
    "print(v.out_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "print(len(result[0][0][0][0][0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11 ('tct')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "14bef272785bfcf432da5f5b24f3510996918aba2097d03905afdcd3ade4004e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
